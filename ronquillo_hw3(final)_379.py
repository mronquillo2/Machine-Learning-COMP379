# -*- coding: utf-8 -*-
"""Ronquillo_HW3(final)_379.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g5aqXyl0Hm27AMRSemLZG2_XnPuyKDV2
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#USE FOR GOOGLE COLAB ONLY
#train = pd.read_csv("/content/train.csv", sep = ',')
#test = pd.read_csv("/content/test.csv", sep = ',')
#gender = pd.read_csv('/content/gender_submission.csv', sep = ',')

#FOR TERMINAL / LOCAL DIRECTORY USAGE
a = open('/Users/melchorronquillo/Desktop/Files/Data/titanic/train.csv')
train = pd.read_csv(a, sep = ',')
b = open('/Users/melchorronquillo/Desktop/Files/Data/titanic/test.csv')
test = pd.read_csv(b, sep = ',')
c = open('/Users/melchorronquillo/Desktop/Files/Data/titanic/gender_submission.csv')
gender = pd.read_csv(c, sep = ',')
#train
#test
#gender


#merge gender with test on ID
test = pd.merge(gender, test, how = 'inner', on = 'PassengerId')
#test



#combine all data
titanic = train.append(test)
#titanic


#randomly shuffle samples in dataset
from sklearn.utils import shuffle
titanic = shuffle(titanic, random_state=123)
#titanic


#get binary values for sex column (men -> 1, female -> 0)
gender = pd.get_dummies(titanic['Sex'])
#gender
titanic = pd.concat((titanic, gender), axis=1)
titanic = titanic.drop(["Sex"], axis=1)
titanic = titanic.drop(["male"], axis=1)
titanic = titanic.rename(columns={"female": "Sex"})
#titanic



#create data frame with variables of interest
titanic2 = titanic[['Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']]
#titanic2



#find which columns have NaN, drop rows of data based on columns with NaN
print(titanic2['Pclass'].isna().sum())
print(titanic2['Sex'].isna().sum())
print(titanic2['Age'].isna().sum())
print(titanic2['SibSp'].isna().sum())
print(titanic2['Parch'].isna().sum())
print(titanic2['Fare'].isna().sum())
print(titanic2['Survived'].isna().sum())

titanic2 = titanic2.dropna(subset=['Age','Fare'])
#titanic2



#scale data, age and fare have value ranges that are way bigger than others
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
titanic_scl = scaler.fit_transform(titanic2.to_numpy())
titanic_scl



#numpy array back to dataframe
titanic_scl = pd.DataFrame(titanic_scl, columns = ['Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived'])
titanic_scl



#split data into train (70%), valdidation (15%), test (15%)
from sklearn.model_selection import train_test_split
train, test, = train_test_split(titanic_scl, test_size = .15, random_state=1)
#train
#test
train, valid = train_test_split(train, test_size = .15, random_state=10)
#train
#valid



#Logistic regression
#create x and y arrays, x with predictors, y with outcomes
x_train = train.iloc[:, 0:6]
y_train = train.iloc[:, 6]
x_valid = valid.iloc[:, 0:6]
y_valid = valid.iloc[:, 6]
#x_train
#y_train

from sklearn.linear_model import LogisticRegression
# model using the default parameters
logregr = LogisticRegression()
# fit the model with data
model = logregr.fit(x_train, y_train)
y_pred = model.predict(x_valid)
#y_pred

from sklearn import metrics
cnf_matrix = metrics.confusion_matrix(y_valid, y_pred)
cnf_matrix

accuracy = metrics.accuracy_score(y_valid, y_pred)
accuracy_percentage = 100 * accuracy
accuracy_percentage



# "smaller values specify stronger regularization", default c = 1.0
logregr_tuned1 = LogisticRegression(C=0.01)

#fit the model with data
model = logregr_tuned1.fit(x_train, y_train)
y_pred = model.predict(x_valid)

cnf_matrix = metrics.confusion_matrix(y_valid, y_pred)
cnf_matrix

accuracy = metrics.accuracy_score(y_valid, y_pred)
accuracy_percentage = 100 * accuracy
accuracy_percentage



#no penalty
logregr_tuned2 = LogisticRegression(penalty = 'none')

#fit the model with data
model = logregr_tuned2.fit(x_train, y_train)
y_pred = model.predict(x_valid)

cnf_matrix = metrics.confusion_matrix(y_valid, y_pred)
cnf_matrix

accuracy = metrics.accuracy_score(y_valid, y_pred)
accuracy_percentage = 100 * accuracy
accuracy_percentage



#"Algorithm to use in the optimization problem, ‘sag’ and ‘saga’ are faster for large ones"
logregr_tuned3 = LogisticRegression(solver = 'sag')

#fit the model with data
model = logregr_tuned3.fit(x_train, y_train)
y_pred = model.predict(x_valid)

cnf_matrix = metrics.confusion_matrix(y_valid, y_pred)
cnf_matrix

accuracy = metrics.accuracy_score(y_valid, y_pred)
accuracy_percentage = 100 * accuracy
accuracy_percentage



#knn
# 1) given a new point
# 2) get distances of all vectors from point
# 3) put all distances in a list
# 4f find the n nunmber of vectors closest to point from list
# 5) assign classification based on majority of classes

x_train_knn = np.array(x_train)
x_valid_knn = np.array(x_valid)
y_train_knn = np.array(y_train)
y_valid_knn = np.array(y_valid)

#k = 1
#list of all predictions made, use for comparison to actual values
pred_class = []
#for every point in the testing set
  #create new list containing distances from test point to every point in training set
for newpoint in x_valid_knn:
  distances = []
  #for every row of data in training data 
    #compute eucladian distance from test point to training point
    #add each distance calucated to distances list
  for i in x_train_knn:
    euclad =  np.sqrt(((newpoint[0]-i[0])**2) + 
                      ((newpoint[1]-i[1])**2) + 
                      ((newpoint[2]-i[2])**2) + 
                      ((newpoint[3]-i[3])**2) + 
                      ((newpoint[4]-i[4])**2) + 
                      ((newpoint[5]-i[5])**2))
    distances.append(euclad)
  #find the smallest value in list and it's index (row number)
    #smallest value = closest distance
  #use index to find specific row of data in outcome array (contains 1 or 0)
  #assign classification of test point to classification of closest train point
  #append predicted classification to predictions list
  mindist = min(distances)
  index = distances.index(mindist)
  #print(index)
  #print(x_train_knn[index])
  #print(y_train_knn[index])
  pred_class.append(y_train_knn[index])
    #find min distance in list
    #find index of row for that distance
    #use index to find class of that value in y
    #classify that point as the class of value from y

#print(pred_class)
#print(y_valid_knn)

print('Misclassified samples: %d' % (y_valid_knn != pred_class).sum())
print('Accuracy = ' + str(float((y_valid_knn == pred_class).sum()) / len(y_valid_knn)*100)+'% with k = 1')




#k = n
k_train_knn = np.array(train)
k_valid_knn = np.array(valid)

k = 3
#list of all predictions made, use for comparison to actual values
kpred_class = []

#iter = 0
#for every point in the testing set
    #create new list containing distances from test point to every point in training set
for newpoint in k_valid_knn:
  #print("Iteration#" + str(iter))
  #iter += 1
  distances = []
  
  #for every row of data in training data
    #compute eucladian distance from test point to training point
    #add each distance calucated to distances list
  for i in k_train_knn:
    euclad =  np.sqrt(((newpoint[0]-i[0])**2) + 
                      ((newpoint[1]-i[1])**2) + 
                      ((newpoint[2]-i[2])**2) + 
                      ((newpoint[3]-i[3])**2) + 
                      ((newpoint[4]-i[4])**2) + 
                      ((newpoint[5]-i[5])**2))
    distances.append(euclad)
  
  ###make distances list an array that can be added to the training array
    ###allows each row to have its respective caluclated distance
    ###data can be sorted without worrying about index of smallest point in
    ### distances list. sorting array by distance can allow k to be any number since
    ### you would just get the first k number of rows from array and its repsective data
  dist = np.array(distances)
  
  ###insert distances array into training array
  arr_with_dist = np.insert(k_train_knn,7,dist,axis=1)
  
  ###sort rows in array by distance from smallest to largest
    ###ex, first 3 rows = 3 closest points to test point
  sort_arr = arr_with_dist[arr_with_dist[:, 7].argsort()]
  #print(sort_arr)
  
  ###for k amount of times to iterate:
    ###create a list that will contain each of the classifications of k rows
  classif_lst = []
  for classif in range(k):
    #print(sort_arr[:,6][classif])
    
    ###add the classifications to list
    classif_lst.append(sort_arr[:,6][classif])
    #print(classif_lst)
  
  ###find the majority classification value
    ###whichever value is counted the most
  ###add prediciton to prediction lisst
  ###use prediction list and compare to actual values of test data
  majority = max(set(classif_lst), key = classif_lst.count)
  #print(majority)
  prediction = majority
  kpred_class.append(prediction)

print('Misclassified samples: %d' % (k_valid_knn[:, 6] != kpred_class).sum())
print('Accuracy = ' + str(float(((k_valid_knn[:, 6] == kpred_class).sum()) / len(k_valid_knn[:, 6])*100)) + '% with k = ' + str(k))




#use this to tune k with different values
  #stop increasing once accuracy decreases
k = 13
kpred_class = []
#iter = 0
for newpoint in k_valid_knn:
  #print("Iteration#" + str(iter))
  #iter += 1
  distances = []
  for i in k_train_knn:
    euclad =  np.sqrt(((newpoint[0]-i[0])**2) + 
                      ((newpoint[1]-i[1])**2) + 
                      ((newpoint[2]-i[2])**2) + 
                      ((newpoint[3]-i[3])**2) + 
                      ((newpoint[4]-i[4])**2) + 
                      ((newpoint[5]-i[5])**2))
    distances.append(euclad)
  dist = np.array(distances)
  arr_with_dist = np.insert(k_train_knn,7,dist,axis=1)
  sort_arr = arr_with_dist[arr_with_dist[:, 7].argsort()]
  #print(sort_arr)
  classif_lst = []
  for classif in range(k):
    #print(sort_arr[:,6][classif])
    classif_lst.append(sort_arr[:,6][classif])
    #print(classif_lst)
  majority = max(set(classif_lst), key = classif_lst.count)
  #print(majority)
  prediction = majority
  kpred_class.append(prediction)

print('Misclassified samples: %d' % (k_valid_knn[:, 6] != kpred_class).sum())
print('Accuracy = ' + str(float(((k_valid_knn[:, 6] == kpred_class).sum()) / len(k_valid_knn[:, 6])*100)) + '% with k = ' + str(k))



#baseline models
x = test.iloc[:, 0:6]
y = test.iloc[:, 6]
from sklearn.dummy import DummyClassifier
for strategy in ['stratified','most_frequent']:
    model_dummy = DummyClassifier(strategy = strategy)
    model_dummy.fit(x,y)
    print("The %s's accuracy is %f"%(strategy,model_dummy.score(x,y)))



#TESTING SET
#knn, k = 3
k_test_knn = np.array(test)
k = 9
kpred_class = []
#iter = 0
for newpoint in k_test_knn:
  #print("Iteration#" + str(iter))
  #iter += 1
  distances = []
  for i in k_train_knn:
    euclad =  np.sqrt(((newpoint[0]-i[0])**2) + 
                      ((newpoint[1]-i[1])**2) + 
                      ((newpoint[2]-i[2])**2) + 
                      ((newpoint[3]-i[3])**2) + 
                      ((newpoint[4]-i[4])**2) + 
                      ((newpoint[5]-i[5])**2))
    distances.append(euclad)
  dist = np.array(distances)
  arr_with_dist = np.insert(k_train_knn,7,dist,axis=1)
  sort_arr = arr_with_dist[arr_with_dist[:, 7].argsort()]
  #print(sort_arr)
  classif_lst = []
  for classif in range(k):
    #print(sort_arr[:,6][classif])
    classif_lst.append(sort_arr[:,6][classif])
    #print(classif_lst)
  majority = max(set(classif_lst), key = classif_lst.count)
  #print(majority)
  prediction = majority
  kpred_class.append(prediction)

print('Misclassified samples: %d' % (k_test_knn[:, 6] != kpred_class).sum())
print('Accuracy = ' + str(float(((k_test_knn[:, 6] == kpred_class).sum()) / len(k_test_knn[:, 6])*100)) + '% with k = ' + str(k))



#TESTING SET
#logistic, penalty = l2, c = 1 (defaults hyperparameters returned best accuracy)
x_test = test.iloc[:, 0:6]
y_test = test.iloc[:, 6]
model = logregr.fit(x_train, y_train)
y_pred = model.predict(x_test)

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix

accuracy = metrics.accuracy_score(y_test, y_pred)
accuracy_percentage = 100 * accuracy
accuracy_percentage

