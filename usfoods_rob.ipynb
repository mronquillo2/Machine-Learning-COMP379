{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e2e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file used to generate outputs for presentations\n",
    "import warnings\n",
    "#use this when printing the output to a pdf\n",
    "warnings.filterwarnings('ignore')\n",
    "#use this when doing coding normally\n",
    "#warnings.filterwarnings('default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9279b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "sae = pd.read_csv('sae.csv')\n",
    "#sae.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "311a1476",
   "metadata": {},
   "source": [
    "Melchors work merging hours and cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fdaea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"# Cases\n",
    "\n",
    "\n",
    "* Change date to date_time object\n",
    "    * Create a new Date column for both datasets\n",
    "    * Allows merging on shared column\n",
    "*  Sort dates in ascending order\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cases = pd.read_csv(\"Cases.csv\")\n",
    "cases\n",
    "\n",
    "# check type, length\n",
    "cases.CLNDR_DT\n",
    "\n",
    "#change CLNDR_DT from object to date_time\n",
    "  # make sure to create new column 'Date' for both datasets so they share common \n",
    "  # for merging\n",
    "\n",
    "cases['Date'] = pd.to_datetime(cases['CLNDR_DT'])\n",
    "cases\n",
    "\n",
    "#sort dates from past to current\n",
    "cases = cases.sort_values(by='CLNDR_DT')\n",
    "cases\n",
    "\n",
    "#drop redundant columns\n",
    "\n",
    "#^Dont need this line\n",
    "#cases = cases.drop(columns=['CLNDR_DT', 'DIV_NM'])\n",
    "#cases\n",
    "\n",
    "# check number of NaN values\n",
    "cases.isna().sum()\n",
    "\n",
    "## needed to drop NaN in order for 'tot_each_day' to be correct number\n",
    "    ## ask why including NaN would yield wrong sum\n",
    "        ## what values does NaN have?\n",
    "\n",
    "cases = cases.dropna()\n",
    "cases\n",
    "\n",
    "## create new dataframe from cases\n",
    "    ## focusing on getting total of eaches selected per day\n",
    "cases_tot_day = pd.DataFrame(cases)\n",
    "\n",
    "## group by Date and BRNCH_CD, sum eaches_selected by each zone into one total each column\n",
    "cases_tot_day['Total_Each_Day'] = pd.DataFrame(cases.groupby(['Date', 'BRNCH_CD'])['EACHES_SELECTED'].transform('sum'))\n",
    "cases_tot_day\n",
    "\n",
    "## New Dataframe with order = Date, Branch, zone, eaches selected, total eaches\n",
    "cases4 = pd.DataFrame(cases, columns = (['Date', 'BRNCH_CD', 'ZONE', 'EACHES_SELECTED', 'Total_Each_Day']))\n",
    "\n",
    "## Change index starting at 1, drop existing index to avoid adding as column\n",
    "cases4 = cases4.reset_index(drop=True)\n",
    "\n",
    "cases4.BRNCH_CD[0]\n",
    "\n",
    "## get ratio of total eaches per zone\n",
    "    ## eaches selected of zone / total eaches\n",
    "    ## add ratio to a list and append to dataset after\n",
    "\n",
    "ratio = []\n",
    "for i in range(0, len(cases4)):\n",
    "  #print(i)\n",
    "  r = cases4.EACHES_SELECTED[i] / cases4.Total_Each_Day[i]\n",
    "  ratio.append(r)\n",
    "\n",
    "ratio[0:10]\n",
    "\n",
    "## add list of ratios to dataframe, should be in order\n",
    "cases4['ratio'] = ratio\n",
    "\n",
    "#type(cases4.ZONE[1])\n",
    "\n",
    "## create a list for every zone (dry, clr, frz)\n",
    "## for every row in dataframe:\n",
    "    ## if the zone of the row == \"DRY\":\n",
    "        ## add the ratio at index i to the \"DRY\" list\n",
    "        ## add zeros to the other lists (ClR, FRZ) (place holder)\n",
    "    ## if zone of row == \"CLR\":\n",
    "        ## add ratio to CLR list\n",
    "        ## add zeros to other lists\n",
    "    ## repeat for FRZ\n",
    "\n",
    "## add lists as columns to dataset\n",
    "\n",
    "dry_ratio = []\n",
    "clr_ratio = []\n",
    "frz_ratio = []\n",
    "\n",
    "for i in range(0, len(cases4)): \n",
    "  if cases4.ZONE[i] == 'DRY':\n",
    "    dry_ratio.append(cases4.ratio[i])\n",
    "    clr_ratio.append(0)\n",
    "    frz_ratio.append(0)\n",
    "  elif cases4.ZONE[i] == 'FRZ':\n",
    "    dry_ratio.append(0)\n",
    "    clr_ratio.append(0)\n",
    "    frz_ratio.append(cases4.ratio[i])\n",
    "  elif cases4.ZONE[i] == 'CLR':\n",
    "    dry_ratio.append(0)\n",
    "    clr_ratio.append(cases4.ratio[i])\n",
    "    frz_ratio.append(0)\n",
    "\n",
    "\n",
    "cases4['dry_ratio'] = dry_ratio\n",
    "cases4['clr_ratio'] = clr_ratio\n",
    "cases4['frz_ratio'] = frz_ratio\n",
    "\n",
    "## sort dataframe by date and branch to see everything together\n",
    "    ## check if math is correct\n",
    "cases4 = cases4.sort_values(by=['Date', 'BRNCH_CD'], ascending = [True, True])\n",
    "\n",
    "## smoosh multiple rows of same branch and date into 1 row\n",
    "    ## aggregate sums, zeros as placeholders will have no effect on ratio\n",
    "cases6 = cases4.groupby(['Date', 'BRNCH_CD', 'Total_Each_Day'], as_index = False).agg('sum')\n",
    "\n",
    "\n",
    "## drop eaches since same as total\n",
    "## drop ratio since all ratios add up to 1 anyway\n",
    "cases_EachPerDay = cases6.drop(columns=['EACHES_SELECTED', 'ratio'])\n",
    "\n",
    "\"\"\"# Hours\n",
    "\n",
    "\n",
    "*   Repeat steps from cases\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "hours = pd.read_csv(\"hours.csv\")\n",
    "\n",
    "hours.REPORT_DATE\n",
    "\n",
    "hours['Date'] = pd.to_datetime(hours['REPORT_DATE'])\n",
    "\n",
    "hours = hours.sort_values(by=['REPORT_DATE', 'BRNCH_CD'], ascending = [True, True])\n",
    "\n",
    "hours = hours.drop(columns=['DIV_NBR', 'REPORT_DATE', 'FULL_MARKET_NAME'])\n",
    "\n",
    "## drop \"NaN\" (UNKOWN)\n",
    "hours = hours[hours.COHORT != 'UNKNOWN']\n",
    "\n",
    "## create datarame in order desired\n",
    "hours_co = pd.DataFrame(hours, columns = ['Date', 'BRNCH_CD', 'COHORT', 'CASES_SELECTED', 'MEASURED_DIRECT_HRS'])\n",
    "\n",
    "## group by Date and branch to get sum of HOURS MEASURED of each cohort as total hours worked at branch on specific day\n",
    "hours_co['Total_Hours'] = pd.DataFrame(hours.groupby(['Date', 'BRNCH_CD'])['MEASURED_DIRECT_HRS'].transform('sum'))\n",
    "\n",
    "\n",
    "## group by Date and branch to get sum of CASES SELECTED of each cohort as total cases selected at branch on specific day\n",
    "hours_co['Total_Cases'] = pd.DataFrame(hours.groupby(['Date', 'BRNCH_CD'])['CASES_SELECTED'].transform('sum'))\n",
    "\n",
    "\n",
    "hours_co = hours_co.reset_index(drop = True)\n",
    "\n",
    "## like hours, get ratio of hours worked per cohort relative to total hours measured at branch on specific day\n",
    "Hrs_PerCo = []\n",
    "for i in range (0, len(hours_co)):\n",
    "  r = hours_co.MEASURED_DIRECT_HRS[i] / hours_co.Total_Hours[i]\n",
    "  Hrs_PerCo.append(r)\n",
    "\n",
    "Hrs_PerCo[0:5]\n",
    "\n",
    "hours_co['Hrs_Pct'] = Hrs_PerCo\n",
    "hours_co\n",
    "\n",
    "## same process of placing respective hours ratios per cohort as eaches per zone in cases\n",
    "    ## 0.000 = placeholder for aggregation\n",
    "A_Hours = []\n",
    "B_Hours = []\n",
    "C_Hours = []\n",
    "\n",
    "for i in range(0, len(hours_co)):\n",
    "  if hours_co.COHORT[i] == 'A':\n",
    "    A_Hours.append(hours_co.Hrs_Pct[i])\n",
    "    B_Hours.append(0)\n",
    "    C_Hours.append(0)\n",
    "  elif hours_co.COHORT[i] == 'C':\n",
    "    A_Hours.append(0)\n",
    "    B_Hours.append(0)\n",
    "    C_Hours.append(hours_co.Hrs_Pct[i])\n",
    "  elif hours_co.COHORT[i] == 'B':\n",
    "    A_Hours.append(0)\n",
    "    B_Hours.append(hours_co.Hrs_Pct[i])\n",
    "    C_Hours.append(0)\n",
    "\n",
    "\n",
    "hours_co['A_HrsPct'] = A_Hours\n",
    "hours_co['B_HrsPct'] = B_Hours\n",
    "hours_co['C_HrsPct'] = C_Hours\n",
    "\n",
    "hours_co\n",
    "\n",
    "## ratio of cases per cohort\n",
    "Cases_PerCo = []\n",
    "for i in range (0, len(hours_co)):\n",
    "  r = hours_co.CASES_SELECTED[i] / hours_co.Total_Cases[i]\n",
    "  Cases_PerCo.append(r)\n",
    "\n",
    "hours_co['Cases_Pct'] = Cases_PerCo\n",
    "hours_co\n",
    "\n",
    "## cases ratio per cohort w/ placeholders\n",
    "A_Cases = []\n",
    "B_Cases = []\n",
    "C_Cases = []\n",
    "\n",
    "for i in range(0, len(hours_co)):\n",
    "  if hours_co.COHORT[i] == 'A':\n",
    "    A_Cases.append(hours_co.Cases_Pct[i])\n",
    "    B_Cases.append(0)\n",
    "    C_Cases.append(0)\n",
    "  elif hours_co.COHORT[i] == 'C':\n",
    "    A_Cases.append(0)\n",
    "    B_Cases.append(0)\n",
    "    C_Cases.append(hours_co.Cases_Pct[i])\n",
    "  elif hours_co.COHORT[i] == 'B':\n",
    "    A_Cases.append(0)\n",
    "    B_Cases.append(hours_co.Cases_Pct[i])\n",
    "    C_Cases.append(0)\n",
    "\n",
    "\n",
    "hours_co['A_Cases'] = A_Cases\n",
    "hours_co['B_Cases'] = B_Cases\n",
    "hours_co['C_Cases'] = C_Cases\n",
    "\n",
    "hours_co\n",
    "\n",
    "hours_CoHrs = hours_co.groupby(['Date', 'BRNCH_CD', 'Total_Hours', 'Total_Cases'], as_index = False).agg('sum')\n",
    "hours_CoHrs\n",
    "\n",
    "\"\"\"# New Datasets\"\"\"\n",
    "\n",
    "cases_EachPerDay\n",
    "\n",
    "## Date = CLNDR_DATE\n",
    "## BRNCH_CD = code referring to sepcific Warehouse\n",
    "\n",
    "## Total_Each_Day = sum of eaches selected by all zones on specific date\n",
    "    ## dry_ratio = proportion of dry products relative to total eaches selected on specific date \n",
    "    ## clr_ratio = proportion of cooler products relative to total eaches selected on specific date\n",
    "    ## frz_ratio = proportion of freezer products relative to total eaches selected on specific date\n",
    "\n",
    "hours_CohortHrsCases = pd.DataFrame(hours_CoHrs, columns = ['Date', 'BRNCH_CD', \n",
    "                                                            'Total_Hours', 'Total_Cases',\n",
    "                                                            'A_HrsPct',\n",
    "                                                            'B_HrsPct', 'C_HrsPct', \n",
    "                                                            'A_Cases', 'B_Cases','C_Cases' \n",
    "                                                            ])\n",
    "hours_CohortHrsCases\n",
    "\n",
    "## Date = REPORT_DATE\n",
    "## BRNCH_CD = code referring to sepcific Warehouse\n",
    "\n",
    "## Total_Hours = sum of hours worked by all cohorts on specific date\n",
    "    ## A_HrsPct = proportion of hours worked by Cohort A relative to total hours on specific date\n",
    "    ## B_HrsPct = proportion of hours worked by Cohort B relative to total hours on specific date\n",
    "    ## C_HrsPct = proportion of hours worked by Cohort C relative to total hours on specific date\n",
    "\n",
    "## Total_Cases = Sum of all cases selected by all cohorts on specific date\n",
    "    ## A_Cases = proportion of cases selected by Cohort A relative to total cases selected on specific date\n",
    "    ## B_Cases = proportion of cases selected by Cohort B relative to total cases selected on specific date\n",
    "    ## C_Cases = proportion of cases selected by Cohort C relative to total cases selected on specific date\n",
    "\n",
    "\"\"\"# Merge\n",
    "* lost a lot of data :/\n",
    "* dont merge yet?\n",
    "\"\"\"\n",
    "\n",
    "Foods = hours_CohortHrsCases.merge(cases_EachPerDay, how = 'inner', on = ['Date', 'BRNCH_CD'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8965090",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "My work adding dates to the dataset and merging Melchor's dataset with the sae dataset. I also created dummy variables for the categorical variables (month, weekday, branch_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb54248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>BRNCH_CD</th>\n",
       "      <th>Total_Hours</th>\n",
       "      <th>Total_Cases</th>\n",
       "      <th>A_HrsPct</th>\n",
       "      <th>B_HrsPct</th>\n",
       "      <th>C_HrsPct</th>\n",
       "      <th>A_Cases</th>\n",
       "      <th>B_Cases</th>\n",
       "      <th>C_Cases</th>\n",
       "      <th>Total_Each_Day</th>\n",
       "      <th>dry_ratio</th>\n",
       "      <th>clr_ratio</th>\n",
       "      <th>frz_ratio</th>\n",
       "      <th>cpmh</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>GO_LIVE_DATE</th>\n",
       "      <th>LABEL_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2Z</td>\n",
       "      <td>1.155000</td>\n",
       "      <td>50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44059</td>\n",
       "      <td>0.393631</td>\n",
       "      <td>0.302799</td>\n",
       "      <td>0.303570</td>\n",
       "      <td>43.290043</td>\n",
       "      <td>Monday</td>\n",
       "      <td>December</td>\n",
       "      <td>2019-05-05</td>\n",
       "      <td>MOBILE TEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2Z</td>\n",
       "      <td>197.701943</td>\n",
       "      <td>35071</td>\n",
       "      <td>0.863320</td>\n",
       "      <td>0.071466</td>\n",
       "      <td>0.065215</td>\n",
       "      <td>0.857461</td>\n",
       "      <td>0.075048</td>\n",
       "      <td>0.067492</td>\n",
       "      <td>40948</td>\n",
       "      <td>0.455920</td>\n",
       "      <td>0.258889</td>\n",
       "      <td>0.285191</td>\n",
       "      <td>177.393300</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>January</td>\n",
       "      <td>2019-05-05</td>\n",
       "      <td>MOBILE TEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>2Z</td>\n",
       "      <td>262.539447</td>\n",
       "      <td>45072</td>\n",
       "      <td>0.785586</td>\n",
       "      <td>0.153591</td>\n",
       "      <td>0.060824</td>\n",
       "      <td>0.789781</td>\n",
       "      <td>0.144058</td>\n",
       "      <td>0.066161</td>\n",
       "      <td>35093</td>\n",
       "      <td>0.420169</td>\n",
       "      <td>0.286382</td>\n",
       "      <td>0.293449</td>\n",
       "      <td>171.677058</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>January</td>\n",
       "      <td>2019-05-05</td>\n",
       "      <td>MOBILE TEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>2Z</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46007</td>\n",
       "      <td>0.442454</td>\n",
       "      <td>0.271133</td>\n",
       "      <td>0.286413</td>\n",
       "      <td>21.649485</td>\n",
       "      <td>Friday</td>\n",
       "      <td>January</td>\n",
       "      <td>2019-05-05</td>\n",
       "      <td>MOBILE TEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>2Z</td>\n",
       "      <td>211.180832</td>\n",
       "      <td>35696</td>\n",
       "      <td>0.820379</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.030811</td>\n",
       "      <td>0.807485</td>\n",
       "      <td>0.165985</td>\n",
       "      <td>0.026530</td>\n",
       "      <td>34949</td>\n",
       "      <td>0.397579</td>\n",
       "      <td>0.304844</td>\n",
       "      <td>0.297576</td>\n",
       "      <td>169.030492</td>\n",
       "      <td>Monday</td>\n",
       "      <td>January</td>\n",
       "      <td>2019-05-05</td>\n",
       "      <td>MOBILE TEAR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date BRNCH_CD  Total_Hours  Total_Cases  A_HrsPct  B_HrsPct  C_HrsPct  \\\n",
       "0 2018-12-31       2Z     1.155000           50  1.000000  0.000000  0.000000   \n",
       "1 2019-01-02       2Z   197.701943        35071  0.863320  0.071466  0.065215   \n",
       "2 2019-01-03       2Z   262.539447        45072  0.785586  0.153591  0.060824   \n",
       "3 2019-01-04       2Z     1.940000           42  1.000000  0.000000  0.000000   \n",
       "4 2019-01-07       2Z   211.180832        35696  0.820379  0.148810  0.030811   \n",
       "\n",
       "    A_Cases   B_Cases   C_Cases  Total_Each_Day  dry_ratio  clr_ratio  \\\n",
       "0  1.000000  0.000000  0.000000           44059   0.393631   0.302799   \n",
       "1  0.857461  0.075048  0.067492           40948   0.455920   0.258889   \n",
       "2  0.789781  0.144058  0.066161           35093   0.420169   0.286382   \n",
       "3  1.000000  0.000000  0.000000           46007   0.442454   0.271133   \n",
       "4  0.807485  0.165985  0.026530           34949   0.397579   0.304844   \n",
       "\n",
       "   frz_ratio        cpmh    weekday     month GO_LIVE_DATE   LABEL_TYPE  \n",
       "0   0.303570   43.290043     Monday  December   2019-05-05  MOBILE TEAR  \n",
       "1   0.285191  177.393300  Wednesday   January   2019-05-05  MOBILE TEAR  \n",
       "2   0.293449  171.677058   Thursday   January   2019-05-05  MOBILE TEAR  \n",
       "3   0.286413   21.649485     Friday   January   2019-05-05  MOBILE TEAR  \n",
       "4   0.297576  169.030492     Monday   January   2019-05-05  MOBILE TEAR  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = False #boolean which determiens whether to drop the dummy variables or not\n",
    "Foods2 = Foods\n",
    "\n",
    "#adding the response variable(s) to the dataset\n",
    "Foods2[\"cpmh\"] = Foods2['Total_Cases']/Foods2['Total_Hours'] #creates the dependent variable we are trying to measure\n",
    "Foods2.replace([np.inf, -np.inf], np.nan, inplace=True) #converting all infinites to na's. will want to come up with a better solution later \n",
    "Foods2 = Foods2.dropna() #drops all na's for now\n",
    "\n",
    "\"\"\" Will probably not want to use this as eaches is not connected to the hours variable\n",
    "epmh_bool = True #set this to true if we want to use epmh as a response as well\n",
    "if(epmh_bool):\n",
    "  Foods2[\"epmh\"] = Foods2['Total_Each_Day']/Foods2['Total_Hours'] #creates the dependent variable we are trying to measure\n",
    "  Foods2.replace([np.inf, -np.inf], np.nan, inplace=True) #converting all infinites to na's. will want to come up with a better solution later \n",
    "  Foods2 = Foods2.dropna() #drops all na's for now\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "weekdays = {0: \"Monday\",\n",
    "            1: \"Tuesday\",\n",
    "            2: \"Wednesday\",\n",
    "            3: \"Thursday\",\n",
    "            4: \"Friday\",\n",
    "            5: \"Saturday\",\n",
    "            6: \"Sunday\"}\n",
    "\n",
    "months =   {1: \"January\",\n",
    "            2: \"February\",\n",
    "            3: \"March\",\n",
    "            4: \"April\",\n",
    "            5: \"May\",\n",
    "            6: \"June\",\n",
    "            7: \"July\",\n",
    "            8: \"August\",\n",
    "            9: \"September\",\n",
    "            10: \"October\",\n",
    "            11: \"November\",\n",
    "            12: \"December\"}\n",
    "            \n",
    "Foods2['weekday'] = Foods2['Date'].apply(lambda x: weekdays[x.weekday()]) #adds the weekday of the particular entry \n",
    "Foods2['month'] = Foods2['Date'].apply(lambda x: months[x.month]) #add the reporting_date month as a separate variable\n",
    "\n",
    "\n",
    "#adds the sae data to the main dataset\n",
    "sae = pd.read_csv('sae.csv')\n",
    "\n",
    "Foods3 = Foods2.merge(sae, how='outer', on='BRNCH_CD')\n",
    "\n",
    "\n",
    "\n",
    "#to add dummy variables to the dataset \n",
    "if(dummies):\n",
    "  Foods3 = pd.concat([Foods2, pd.get_dummies(Foods2['BRNCH_CD'])], axis=1)\n",
    "  Foods3 = pd.concat([Foods2, pd.get_dummies(Foods2['weekday'])], axis=1)\n",
    "  Foods3 = pd.concat([Foods2, pd.get_dummies(Foods2['month'])], axis=1)\n",
    "  Foods3 = Foods2.drop([\"BRNCH_CD\", 'Date', 'month', 'weekday'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Foods3.to_csv('model_data.csv', index=False)\n",
    "Foods3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb6cbfc4",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "foods4 = pd.read_csv(\"model_data.csv\")\n",
    "foods4 = foods4.drop(columns=['Total_Cases', 'Total_Hours', 'C_HrsPct', 'C_Cases', 'frz_ratio'])\n",
    "foods4 = foods4.drop(columns=['GO_LIVE_DATE']) #dropping go_live_date for now\n",
    "\n",
    "foods4.columns\n",
    "\n",
    "#converting variables to work with the sklearn model \n",
    "foods4['Date'] = pd.to_datetime(foods4['Date']).astype(int)\n",
    "\n",
    "\n",
    "foods4 = pd.concat([foods4, pd.get_dummies(foods4['BRNCH_CD'])], axis=1)\n",
    "foods4 = pd.concat([foods4, pd.get_dummies(foods4['weekday'])], axis=1)\n",
    "foods4 = pd.concat([foods4, pd.get_dummies(foods4['month'])], axis=1)\n",
    "foods4 = pd.concat([foods4, pd.get_dummies(foods4['LABEL_TYPE'])], axis=1)\n",
    "foods4 = foods4.drop([\"BRNCH_CD\", 'Date', 'month', 'weekday', 'LABEL_TYPE'], axis=1)\n",
    "\n",
    "\n",
    "#splitting the data up for cross validation\n",
    "x = foods4['cpmh'].values\n",
    "y = foods4.drop(columns='cpmh').values\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3) \n",
    "\n",
    "#scores = cross_val_score() #can use this function to score our cross validation\n",
    "\n",
    "#converting the data to create a model and then creating the model\n",
    "x_train = np.array(x_train).reshape(-1,1)\n",
    "\n",
    "reg = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "x_test = np.array(x_test).reshape(-1,1)\n",
    "reg.score(x_test, y_test)\n",
    "\n",
    "x = np.array(x).reshape(-1, 1)\n",
    "cross_val_score(reg, x,y) #finding r^2 scores using cross-validation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "368c03b4",
   "metadata": {},
   "source": [
    "My work seeing if adding weather data helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from meteostat import Daily, Stations\n",
    "from geopy import Nominatim\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "\n",
    "foods_w = Foods3[Foods3['cpmh'] < 300] #dropping all cpmh over 300 just for this analysis\n",
    "\n",
    "\n",
    "locator = Nominatim(user_agent='myGeocoder')\n",
    "#loc = locator.geocode('PITTSTON SYSTEMS')\n",
    "\n",
    "\n",
    "div_nm = {} #dictionary that stores the division name and its cooresponding branch code to easily add weather data later on\n",
    "for n in cases['DIV_NM'].unique():\n",
    "    div_nm[n] = cases.where(cases['DIV_NM'] == n)['BRNCH_CD'].dropna(how='all').iloc[0]\n",
    "\n",
    "#create a dictionary which stores the info from div_nm in a more useful way to create a dataframe\n",
    "div = {'name': div_nm.keys(), 'BRNCH_CD':div_nm.values()}\n",
    "div_df = pd.DataFrame(data=div)\n",
    "\n",
    "#get the location data for each warehouse\n",
    "loc = []\n",
    "for d in tqdm(div_nm):\n",
    "    loc.append(locator.geocode(d))\n",
    "\n",
    "loc1 = []\n",
    "for l in loc:\n",
    "    try:\n",
    "        loc1.append(l[1])\n",
    "    except:\n",
    "        loc1.append(0)\n",
    "\n",
    "div_df['location'] = loc1 \n",
    "\n",
    "#drop the division name and then merge the two databases together \n",
    "div_df = div_df.drop('name', axis=1)\n",
    "foods_w = foods_w.merge(div_df, how='inner', on='BRNCH_CD')\n",
    "\n",
    "#get the station info for each location and get the weather for the day in question\n",
    "station = Stations()\n",
    "stat = {}\n",
    "for l in foods_w['location'].unique():\n",
    "    try:\n",
    "        stat[l] = station.nearby(l[0], l[1]).fetch(1)\n",
    "    except:\n",
    "        stat[l] = 0 #assigns a 0 for each location that was unable to be determined prior\n",
    "        \n",
    "\n",
    "foods_w = foods_w[foods_w['location'] != 0] #drops rows where the location is equal to 0 \n",
    "\n",
    "it = foods_w.itertuples(index=False)\n",
    "weather = []\n",
    "num = 0\n",
    "for f in tqdm(it, total=len(foods_w)):\n",
    "    num = num+1\n",
    "    if(num > len(foods_w)): #only allows the loop to run 100 times for now\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            d = Daily(stat[f[19]], f[0], f[0]).fetch()\n",
    "            d['BRNCH_CD'] = f[1]\n",
    "            weather.append(d)\n",
    "        except:\n",
    "            weather.append(0) #adds a zero for the observations where weather info is unavailable\n",
    "\n",
    "#d = Daily()\n",
    "#d['tavg'][0] #how to access the individual values from the daily info\n",
    "\n",
    "#TODO have to figure out which weather values (temperature, percipitation, etc... are valuable for us to have in our model)\n",
    "\n",
    "#Foods3.to_csv('model_data.csv', index=False)\n",
    "\n",
    "#https://dev.meteostat.net/python/daily.html#api #site with all of the info on the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7043dc5b",
   "metadata": {},
   "source": [
    "Serializing the weather data so i dont have to fetch the weather data agian (this saves the python objects to a file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e98f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#writes all of the weather data to a file \n",
    "with open(\"weather.pickle\", \"wb\") as outfile:\n",
    "    pickle.dump(weather, outfile)\n",
    "\n",
    "\n",
    "#with open(\"weather.pickle\", \"rb\") as infile:\n",
    " #   weather1 = pickle.load(infile)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "429969ee",
   "metadata": {},
   "source": [
    "Doing analysis on the weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"weather.pickle\", \"rb\") as infile:\n",
    "    weather1 = pickle.load(infile)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weather1 = weather\n",
    "weather_pd = pd.concat(weather1)\n",
    "\n",
    "\n",
    "\n",
    "foods_w1 = foods_w\n",
    "foods_w1 = foods_w1.reset_index()\n",
    "weather_pd = pd.concat(weather1)\n",
    "weather_pd['Date'] = weather_pd.index\n",
    "weather_pd.index = range(0,len(weather_pd),1)\n",
    "\n",
    "foods_w1 = pd.merge(foods_w1, weather_pd, on=['BRNCH_CD', 'Date'])\n",
    "\n",
    "\n",
    "foods_w1.to_csv('weather_data.csv', index=False)\n",
    "\n",
    "\n",
    "foods_w1.columns\n",
    "\n",
    "\n",
    "\n",
    "b = pd.DataFrame()\n",
    "b['tavg'] = foods_w1['tavg']\n",
    "b['cpmh'] = foods_w1\n",
    "\n",
    "plt.boxplot()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841189f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"'Preliminary regression testing'\n",
    "\n",
    "#creating a base linear model with cpmh as the response \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3) \n",
    "reg = LinearRegression().fit(x_train, y_train)\n",
    "pred = reg.predict(x_test)\n",
    "mean_squared_error(y_test, pred)\n",
    "r2_score(y_test, pred)\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\" \"Testing Code\"\n",
    "Foods2['weekday'] = Foods2['Date'].apply(lambda x: weekdays[x.weekday()]) #adds the weekday of the particular entry \n",
    "\n",
    "x = datetime(2022,12,16)\n",
    "d = datetime(2023, 1, 8)\n",
    "l = 0\n",
    "\n",
    "\n",
    "x.isoweekday()\n",
    "weekdays[x.weekday()]\n",
    "\n",
    "type(Foods.Date.iloc[0])\n",
    "Foods2.columns\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"'useful code'\n",
    "cases.where(cases['DIV_NM'] == h)['BRNCH_CD'].dropna(how='all').iloc[0] #line to get the unique branch code for each division name\n",
    "#code to fetch weather data \n",
    "f = foods_w.iloc[1500]\n",
    "s = station.nearby(f[19][0], f[19][1]).fetch(1)\n",
    "d = Daily(s, f[0], f[0]).fetch()\n",
    "d['tavg'][0] #how to access the individual values from the daily info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
